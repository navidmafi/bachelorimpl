{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import silence_tensorflow.auto\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "import keras\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = 1\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "session = tf.compat.v1.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 512  # height\n",
    "W = 512  # width\n",
    "C = 3  # channels\n",
    "h = 12  # heads\n",
    "\n",
    "P = 16  # patch size\n",
    "assert H == W\n",
    "assert H % P == 0\n",
    "\n",
    "D_model: int = 1024  # transformer latent dim\n",
    "D_head: int = 128  # dim of each head\n",
    "D_fcn: int = 4096  # FCN hidden dim\n",
    "num_layers: int = 6  # transformer depth\n",
    "N: int = (H * W) // (P * P)  # Number of patches\n",
    "BS = 2\n",
    "\n",
    "# assert h * D_head == D_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLOAT = tf.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_validate(file_path):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_image(img, channels=C, expand_animations=False)\n",
    "    img = tf.divide(tf.cast(img, dtype=FLOAT), 255.0)\n",
    "    is_valid = tf.reduce_all(tf.equal(tf.shape(img), tf.constant((H, W, C))))\n",
    "\n",
    "    return img, is_valid\n",
    "\n",
    "\n",
    "dataset_path = \"/mnt/Data/ML/datasets/portraits\"\n",
    "num_samples = 10000\n",
    "\n",
    "\n",
    "all_files = [\n",
    "    os.path.join(dataset_path, f)\n",
    "    for f in os.listdir(dataset_path)\n",
    "    if f.endswith((\".jpg\", \".png\"))\n",
    "]\n",
    "random.shuffle(all_files)\n",
    "selected_files = all_files[:num_samples]\n",
    "dataset = tf.data.Dataset.from_tensor_slices(selected_files)\n",
    "dataset = dataset.map(load_and_validate)\n",
    "dataset = dataset.filter(lambda img, is_valid: is_valid)  # Keep valid images\n",
    "dataset = dataset.map(lambda img, is_valid: img)  # remove unused feature\n",
    "dataset = dataset.map(lambda img: tf.ensure_shape(img, (H, W, C)))\n",
    "print(f\"Total files: {len(selected_files)}\")\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    valid_count = dataset.reduce(tf.constant(0, dtype=tf.int32), lambda x, _: x + 1).numpy()\n",
    "\n",
    "print(f\"Valid images count: {valid_count}\")\n",
    "assert valid_count, \"Everything's gone\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_img(img):\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    plt.imshow(tf.squeeze(img).numpy(), cmap=\"gray\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def viz_mask(mask):\n",
    "    plt.imshow(tf.squeeze(mask).numpy(), cmap=\"gray\", vmin=0, vmax=1)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "def random_visibility_mask():\n",
    "    x1 = tf.random.uniform(shape=(), minval=0, maxval=W - 100, dtype=tf.int32)\n",
    "    y1 = tf.random.uniform(shape=(), minval=0, maxval=H - 100, dtype=tf.int32)\n",
    "    x2 = tf.random.uniform(shape=(), minval=x1 + 100, maxval=W + 1, dtype=tf.int32)\n",
    "    y2 = tf.random.uniform(shape=(), minval=y1 + 100, maxval=H + 1, dtype=tf.int32)\n",
    "    # tf.print(x1,x2,y1,y2)\n",
    "\n",
    "    mask = tf.ones((H, W), dtype=tf.bool)\n",
    "    mask = tf.tensor_scatter_nd_update(\n",
    "        mask,\n",
    "        indices=tf.stack(\n",
    "            [\n",
    "                tf.repeat(tf.range(y1, y2), x2 - x1),\n",
    "                tf.tile(tf.range(x1, x2), [y2 - y1]),\n",
    "            ],\n",
    "            axis=-1,\n",
    "        ),\n",
    "        updates=tf.zeros([(y2 - y1) * (x2 - x1)], dtype=tf.bool),\n",
    "    )\n",
    "    return tf.expand_dims(mask, -1)  # expand channel wise\n",
    "\n",
    "\n",
    "# def mask_area(mask):\n",
    "#     return tf.reduce_sum(tf.cast(mask, tf.int32))\n",
    "\n",
    "\n",
    "def extract_patches(image: tf.Tensor) -> tf.Tensor:\n",
    "    \"R^{BS x H x W x C} -> R^{BS x N x P^2 x C}\"\n",
    "    # print(image.dtype)\n",
    "\n",
    "    patches: tf.Tensor = tf.image.extract_patches(\n",
    "        images=image,  # Add batch dim\n",
    "        sizes=[1, P, P, 1],  # Patch size\n",
    "        strides=[1, P, P, 1],  # Step size\n",
    "        rates=[1, 1, 1, 1],  # No dilation\n",
    "        padding=\"VALID\",\n",
    "    )\n",
    "    BS, H_prime, W_prime, _ = tf.unstack(tf.shape(patches))\n",
    "\n",
    "    # Reshape patches to [BS, H' * W', P*P, C]\n",
    "    patches = tf.reshape(patches, [BS, H_prime * W_prime, P * P, -1])\n",
    "\n",
    "    return patches\n",
    "\n",
    "\n",
    "def patches_to_imgs(patches: tf.Tensor) -> tf.Tensor:\n",
    "    \"R^{BS x N x P^2 x C} -> R^{BS x H x W x C}\"\n",
    "    BS = tf.shape(patches)[0]\n",
    "    grid_size = H // P  # same as W // P\n",
    "\n",
    "    patches = tf.reshape(patches, [BS, grid_size, grid_size, P, P, C])\n",
    "    patches = tf.transpose(patches, perm=[0, 1, 3, 2, 4, 5])\n",
    "\n",
    "    image = tf.reshape(patches, [BS, grid_size * P, grid_size * P, C])\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def process_mask(mask: tf.Tensor):\n",
    "    \"R^{BS x H x W} -> tuple[R^{BS x N}, R^{BS x N x N}]\"\n",
    "    # viz_mask(mask)\n",
    "    BS = tf.shape(mask)[0]\n",
    "    mask_pooled = tf.nn.max_pool2d(\n",
    "        tf.cast(\n",
    "            tf.logical_not(mask), dtype=tf.int8\n",
    "        ),  # insane shit happaned here with mask_inverted\n",
    "        ksize=[P, P],\n",
    "        strides=[P, P],\n",
    "        padding=\"VALID\",\n",
    "    )\n",
    "    mask_pooled = tf.logical_not(tf.cast(mask_pooled, tf.bool))\n",
    "    # viz_mask(mask_pooled)\n",
    "    mask_pooled = tf.reshape(mask_pooled, [BS, N])\n",
    "    mask_expanded = tf.expand_dims(mask_pooled, axis=1)  # (BS, 1, N)\n",
    "    mask_expanded = tf.tile(mask_expanded, [1, N, 1])  # (BS, N, N)\n",
    "    A = tf.where(\n",
    "        mask_expanded,\n",
    "        tf.constant(0.0, dtype=FLOAT),  # zero penanly\n",
    "        tf.constant(-float(\"inf\"), dtype=FLOAT),  # inf penalty\n",
    "    )\n",
    "    # tf.print(tf.shape(A))\n",
    "\n",
    "    return mask_pooled, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__(dtype=FLOAT)  # wtf\n",
    "        self.proj = keras.layers.Dense(D_model, dtype=FLOAT)  # (PÂ² * C) -> D_model\n",
    "        self.positional_embedding = self.add_weight(\n",
    "            shape=(1, N, D_model),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "            name=\"positional_embedding\",\n",
    "            dtype=FLOAT,\n",
    "        )\n",
    "\n",
    "    # def build(self, input_shape):\n",
    "    #     self.call(tf.zeros((BS, N , P**2 * C)))\n",
    "    #     self.built = True\n",
    "\n",
    "    def call(self, patches_flat: tf.Tensor):\n",
    "        # R^{N x (P^2 . C)} -> R^{N x D_model}\n",
    "        assert patches_flat.dtype == FLOAT\n",
    "        X = self.proj(patches_flat)  # (N, D_model)\n",
    "        assert X.dtype == FLOAT\n",
    "        X += self.positional_embedding  # (N, D_model)\n",
    "        assert X.dtype == FLOAT\n",
    "        return X\n",
    "\n",
    "\n",
    "class MultiHeadAttention(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__(dtype=FLOAT)\n",
    "        # Project to h * D_head dimensions\n",
    "        self.W_Q = keras.layers.Dense(h * D_head, dtype=FLOAT)\n",
    "        self.W_K = keras.layers.Dense(h * D_head, dtype=FLOAT)\n",
    "        self.W_V = keras.layers.Dense(h * D_head, dtype=FLOAT)\n",
    "        # Project back to D_model\n",
    "        self.W_O = keras.layers.Dense(D_model, dtype=FLOAT)\n",
    "\n",
    "    def call(self, X, A):\n",
    "        # X: R^{BS x N x D_model}\n",
    "        # A: R^{BS x N x N}\n",
    "        # returns: R^{BS x N x D_model}\n",
    "\n",
    "        # In the standard implementation, each head has its own separate projection matrices. However, a common optimization is to project the input into h * D_head dimensions (which is D_model) with a single large projection, then split into h heads. So, if D_model = h * D_head, then using a Dense(D_model) for Q, K, V and then splitting into h heads each of D_head is equivalent to having h separate projections. This is a standard approach because it's more efficient to compute all heads in parallel with a single matrix multiplication rather than h separate ones.\n",
    "        # So the optimal way is to use combined projections.\n",
    "        Q = self.W_Q(X)  # (BS, N, h * D_head)\n",
    "        K = self.W_K(X)  # (BS, N, h * D_head)\n",
    "        V = self.W_V(X)  # (BS, N, h * D_head)\n",
    "        \n",
    "\n",
    "        Q = tf.reshape(Q, (-1, N, h, D_head))  # (BS, N, h, D_head)\n",
    "        K = tf.reshape(K, (-1, N, h, D_head))\n",
    "        V = tf.reshape(V, (-1, N, h, D_head))\n",
    "\n",
    "        # Transpose for attention computation\n",
    "        Q = tf.transpose(Q, [0, 2, 1, 3])  # (BS, h, N, D_head)\n",
    "        K = tf.transpose(K, [0, 2, 1, 3])\n",
    "        V = tf.transpose(V, [0, 2, 1, 3])\n",
    "        # scaled dot-product attention\n",
    "        attn_scores = tf.matmul(Q, K, transpose_b=True)  # (BS, h, N, N)\n",
    "        attn_scores /= tf.math.sqrt(\n",
    "            tf.cast(D_head, attn_scores.dtype)\n",
    "        )  # scale by sqrt(D_head)\n",
    "\n",
    "        A = tf.expand_dims(A, 1)  # (BS, 1, N, N)\n",
    "        attn_scores += A  # Broadcast to all heads\n",
    "\n",
    "        attn_weights = tf.nn.softmax(attn_scores, axis=-1)  # (BS, h, N, N)\n",
    "\n",
    "        output = tf.matmul(attn_weights, V)  # (BS, h, N, D_head)\n",
    "        output = tf.transpose(output, [0, 2, 1, 3])  # (BS, N, h, D_head)\n",
    "        output = tf.reshape(output, (-1, N, h * D_head))  # (BS, N, h * D_head)\n",
    "        output = self.W_O(output)  # (BS, N, D_model)\n",
    "        return output\n",
    "\n",
    "\n",
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__(dtype=FLOAT)\n",
    "        self.attn = MultiHeadAttention()\n",
    "        self.norm1 = keras.layers.LayerNormalization(dtype=FLOAT)\n",
    "        self.norm2 = keras.layers.LayerNormalization(dtype=FLOAT)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                keras.layers.Dense(D_fcn, activation=\"gelu\", dtype=FLOAT),\n",
    "                keras.layers.Dense(D_model, dtype=FLOAT),\n",
    "                # keras.layers.Dropout(0.1, dtype=FLOAT),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    def call(self, X, A):\n",
    "        \"R^{N x D_model} -> R^{N x D_model}\"\n",
    "        X = self.norm1(X + self.attn(X, A))\n",
    "        X = self.norm2(X + self.ffn(X))\n",
    "        return X\n",
    "\n",
    "\n",
    "class Decoder(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__(dtype=FLOAT)\n",
    "        self.proj = keras.layers.Dense(P * P * C, dtype=FLOAT)\n",
    "\n",
    "    def call(self, X):\n",
    "        \"R^{BS x N x D_model} -> R^{BS x N x P x P x C}\"\n",
    "        BS = tf.shape(X)[0]\n",
    "        X = self.proj(X)\n",
    "        X = tf.reshape(X, (BS, N, P, P, C))\n",
    "        return X\n",
    "\n",
    "\n",
    "class ImageInpaintingTransformer(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__(dtype=FLOAT)\n",
    "        self.embed = PatchEmbedding()\n",
    "        self.transformer_blocks = [TransformerBlock() for _ in range(num_layers)]\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        BS = input_shape[0]\n",
    "        dummy_images = tf.zeros((BS, H, W, C), dtype=FLOAT)  # THIS WASTED 40 MINUTES\n",
    "        dummy_masks = tf.stack([random_visibility_mask() for _ in range(BS)])\n",
    "        self.call(dummy_images, dummy_masks)\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, image, mask):\n",
    "        patches = extract_patches(image)\n",
    "        mask_pooled, A = process_mask(mask)\n",
    "        BS = tf.shape(patches)[0]\n",
    "        patches_flat = tf.reshape(patches, [BS, N, P**2 * C])\n",
    "        # tf.print(tf.shape(patches_flat))\n",
    "        X = self.embed(patches_flat)\n",
    "        for block in self.transformer_blocks:\n",
    "            X = block(X, A)\n",
    "        reconstructed_patches = self.decoder(X)  # R^{BS x N x P x P x C}\n",
    "        return patches_to_imgs(reconstructed_patches)\n",
    "\n",
    "model = ImageInpaintingTransformer()\n",
    "model.build((BS, H, W, C))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tests patching and depatching\n",
    "\n",
    "# img = next(iter(dataset.take(1)))\n",
    "\n",
    "# viz_img(img)\n",
    "# patched = extract_patches(tf.expand_dims(img ,0))\n",
    "# tf.print(tf.shape(patched))\n",
    "\n",
    "# recreated_img = patches_to_imgs(patched)\n",
    "# viz_img(recreated_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sample(image):\n",
    "    mask = random_visibility_mask()\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "ds_masks = dataset.map(prepare_sample)\n",
    "train_count = int(valid_count * 0.8)\n",
    "test_count = int(valid_count * 0.1)\n",
    "val_count = valid_count - train_count - test_count\n",
    "\n",
    "train_ds = ds_masks.take(train_count).batch(BS)\n",
    "test_ds = ds_masks.skip(train_count).take(test_count).batch(BS)\n",
    "val_ds = ds_masks.skip(train_count + test_count).take(val_count).batch(BS)\n",
    "print(train_count, test_count,val_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def costfunc(y_true: tf.Tensor, y_pred: tf.Tensor, obsvmask: tf.Tensor):\n",
    "    inpaintmask = tf.cast(tf.logical_not(obsvmask), FLOAT)\n",
    "    absolute_errors = tf.abs(tf.subtract(y_true, y_pred))\n",
    "    masked_errors = tf.multiply(absolute_errors, inpaintmask)\n",
    "    sum_masked_errors = tf.reduce_sum(masked_errors)\n",
    "    area = tf.reduce_sum(inpaintmask)\n",
    "    return sum_masked_errors / (area + keras.backend.epsilon())\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-6)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(image: tf.Tensor, mask: tf.Tensor):\n",
    "    with tf.GradientTape() as tape:\n",
    "        reconstructed_img = model(image, mask)  # N x P x P x C\n",
    "        loss = costfunc(image, reconstructed_img, mask)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def val_step(image: tf.Tensor, mask: tf.Tensor):\n",
    "    reconstructed_img = model(image, mask, training = False)\n",
    "    loss = costfunc(image, reconstructed_img, mask)\n",
    "    return loss\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "print(\"Starting training\")\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    steps = 0\n",
    "    pbar = tqdm(train_ds, desc=f\"Epoch {epoch+1}\", unit=\"batch\", total=train_count // BS)\n",
    "    for image_batch, mask_batch in pbar:\n",
    "        loss = train_step(image_batch, mask_batch)\n",
    "        epoch_loss += loss\n",
    "        steps += 1\n",
    "        # Dynamically update the tqdm bar without spamming stdout\n",
    "        pbar.set_postfix(loss=f\"{loss:.4f}\")\n",
    "    train_loss = epoch_loss / steps\n",
    "\n",
    "    val_loss_total = 0.0\n",
    "    val_steps = 0\n",
    "    pbar_val = tqdm(val_ds, desc=f\"Epoch {epoch+1} Validation\", unit=\"batch\", total=val_count // BS)\n",
    "    for val_image_batch, val_mask_batch in pbar_val:\n",
    "        loss = val_step(val_image_batch, val_mask_batch)\n",
    "        val_loss_total += loss\n",
    "        val_steps += 1\n",
    "        pbar_val.set_postfix(loss=f\"{loss:.4f}\")\n",
    "    avg_val_loss = val_loss_total / val_steps\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Summary: Train Loss = {train_loss:.4f} | Validation Loss = {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"b2.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_obsv_mask(image: tf.Tensor, obvmask: tf.Tensor) -> tf.Tensor:\n",
    "    return tf.multiply(image, tf.cast(obvmask, FLOAT))\n",
    "\n",
    "\n",
    "def viz_grid(batch: tf.Tensor):\n",
    "    batch_size: int = batch.shape[0]  # type: ignore\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=batch_size, figsize=(15, 15))\n",
    "    for i in range(batch_size):\n",
    "        # Original image\n",
    "        axes[i].imshow(tf.cast(batch[i], dtype=tf.float32).numpy())  # type: ignore\n",
    "        axes[i].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def reconstruct(original: tf.Tensor, reconstruct: tf.Tensor, obvmask: tf.Tensor):\n",
    "    return tf.add(\n",
    "        tf.multiply(tf.cast(obvmask, FLOAT), original),\n",
    "        tf.multiply(tf.cast(tf.logical_not(obvmask), FLOAT), reconstruct),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualitative Eval\n",
    "# visualize_unbatched_dataset(test_ds, 5)\n",
    "\n",
    "img, obvmask = next(iter(val_ds.take(1)))\n",
    "viz_grid(img)\n",
    "viz_grid(apply_obsv_mask(img, obvmask))\n",
    "reconstructed = reconstruct(img, model(img, obvmask), obvmask)\n",
    "viz_grid(reconstructed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
