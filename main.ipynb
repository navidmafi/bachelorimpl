{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738428489.235415   40186 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738428489.241865   40186 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import silence_tensorflow.auto\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "import keras\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 512  # height\n",
    "W = 512  # width\n",
    "C = 3  # channels\n",
    "h = 16  # heads\n",
    "\n",
    "P = 16  # patch size\n",
    "assert H == W\n",
    "assert H % P == 0\n",
    "\n",
    "D_model: int = 512  # transformer latent dim\n",
    "D_head: int = 128  # dim of each head\n",
    "D_fcn: int = 2048  # FCN hidden dim\n",
    "num_layers: int = 4  # transformer depth\n",
    "N: int = (H * W) // (P * P)  # Number of patches\n",
    "BS = 4\n",
    "\n",
    "# assert h * D_head == D_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLOAT = tf.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1738428496.132556   40186 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2181 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 5000\n",
      "Valid images count: 5000\n"
     ]
    }
   ],
   "source": [
    "def load_and_validate(file_path):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_image(img, channels=C, expand_animations=False)\n",
    "    img = tf.divide(tf.cast(img, dtype=FLOAT), 255.0)\n",
    "    is_valid = tf.reduce_all(tf.equal(tf.shape(img), tf.constant((H, W, C))))\n",
    "\n",
    "    return img, is_valid\n",
    "\n",
    "\n",
    "dataset_path = \"/mnt/Data/ML/datasets/portraits\"\n",
    "num_samples = 5000\n",
    "\n",
    "\n",
    "all_files = [\n",
    "    os.path.join(dataset_path, f)\n",
    "    for f in os.listdir(dataset_path)\n",
    "    if f.endswith((\".jpg\", \".png\"))\n",
    "]\n",
    "random.shuffle(all_files)\n",
    "selected_files = all_files[:num_samples]\n",
    "dataset = tf.data.Dataset.from_tensor_slices(selected_files)\n",
    "dataset = dataset.map(load_and_validate, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "dataset = dataset.filter(lambda img, is_valid: is_valid)  # Keep valid images\n",
    "dataset = dataset.map(lambda img, is_valid: img)  # remove unused feature\n",
    "dataset = dataset.map(lambda img: tf.ensure_shape(img, (H, W, C)))\n",
    "print(f\"Total files: {len(selected_files)}\")\n",
    "valid_count = dataset.reduce(tf.constant(0), lambda x, _: x + 1).numpy()\n",
    "print(f\"Valid images count: {valid_count}\")\n",
    "assert valid_count, \"Everything's gone\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_img(img):\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    plt.imshow(tf.squeeze(img).numpy(), cmap=\"gray\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def viz_mask(mask):\n",
    "    plt.imshow(tf.squeeze(mask).numpy(), cmap=\"gray\", vmin=0, vmax=1)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "def random_visibility_mask():\n",
    "    x1 = tf.random.uniform(shape=(), minval=0, maxval=W - 100, dtype=tf.int32)\n",
    "    y1 = tf.random.uniform(shape=(), minval=0, maxval=H - 100, dtype=tf.int32)\n",
    "    x2 = tf.random.uniform(shape=(), minval=x1 + 100, maxval=W + 1, dtype=tf.int32)\n",
    "    y2 = tf.random.uniform(shape=(), minval=y1 + 100, maxval=H + 1, dtype=tf.int32)\n",
    "    # tf.print(x1,x2,y1,y2)\n",
    "\n",
    "    mask = tf.ones((H, W), dtype=tf.bool)\n",
    "    mask = tf.tensor_scatter_nd_update(\n",
    "        mask,\n",
    "        indices=tf.stack(\n",
    "            [\n",
    "                tf.repeat(tf.range(y1, y2), x2 - x1),\n",
    "                tf.tile(tf.range(x1, x2), [y2 - y1]),\n",
    "            ],\n",
    "            axis=-1,\n",
    "        ),\n",
    "        updates=tf.zeros([(y2 - y1) * (x2 - x1)], dtype=tf.bool),\n",
    "    )\n",
    "    return tf.expand_dims(mask, -1)  # expand channel wise\n",
    "\n",
    "\n",
    "def mask_area(mask):\n",
    "    return tf.reduce_sum(tf.cast(mask, tf.int32))\n",
    "\n",
    "\n",
    "def extract_patches(image: tf.Tensor) -> tf.Tensor:\n",
    "    \"R^{BS x H x W x C} -> R^{BS x N x P^2 x C}\"\n",
    "    # print(image.dtype)\n",
    "\n",
    "    patches: tf.Tensor = tf.image.extract_patches(\n",
    "        images=image,  # Add batch dim\n",
    "        sizes=[1, P, P, 1],  # Patch size\n",
    "        strides=[1, P, P, 1],  # Step size\n",
    "        rates=[1, 1, 1, 1],  # No dilation\n",
    "        padding=\"VALID\",\n",
    "    )\n",
    "    BS, H_prime, W_prime, _ = tf.unstack(tf.shape(patches))\n",
    "\n",
    "    # Reshape patches to [BS, H' * W', P*P, C]\n",
    "    patches = tf.reshape(patches, [BS, H_prime * W_prime, P * P, -1])\n",
    "\n",
    "    return patches\n",
    "\n",
    "\n",
    "def patches_to_imgs(patches: tf.Tensor) -> tf.Tensor:\n",
    "    \"R^{BS x N x P^2 x C} -> R^{BS x H x W x C}\"\n",
    "    BS = tf.shape(patches)[0]\n",
    "    grid_size = H // P  # same as W // P\n",
    "\n",
    "    patches = tf.reshape(patches, [BS, grid_size, grid_size, P, P, C])\n",
    "    patches = tf.transpose(patches, perm=[0, 1, 3, 2, 4, 5])\n",
    "\n",
    "    image = tf.reshape(patches, [BS, grid_size * P, grid_size * P, C])\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def process_mask(mask: tf.Tensor):\n",
    "    \"R^{BS x H x W} -> tuple[R^{BS x N}, R^{BS x N x N}]\"\n",
    "    # viz_mask(mask)\n",
    "    BS = tf.shape(mask)[0]\n",
    "    mask_pooled = tf.nn.max_pool2d(\n",
    "        tf.cast(\n",
    "            tf.logical_not(mask), dtype=tf.int8\n",
    "        ),  # insane shit happaned here with mask_inverted\n",
    "        ksize=[P, P],\n",
    "        strides=[P, P],\n",
    "        padding=\"VALID\",\n",
    "    )\n",
    "    mask_pooled = tf.logical_not(tf.cast(mask_pooled, tf.bool))\n",
    "    # viz_mask(mask_pooled)\n",
    "    mask_pooled = tf.reshape(mask_pooled, [BS, N])\n",
    "    mask_expanded = tf.expand_dims(mask_pooled, axis=1)  # (BS, 1, N)\n",
    "    mask_expanded = tf.tile(mask_expanded, [1, N, 1])  # (BS, N, N)\n",
    "    A = tf.where(\n",
    "        mask_expanded,\n",
    "        tf.constant(0.0, dtype=FLOAT),  # zero penanly\n",
    "        tf.constant(-float(\"inf\"), dtype=FLOAT),  # inf penalty\n",
    "    )\n",
    "    # tf.print(tf.shape(A))\n",
    "\n",
    "    return mask_pooled, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"image_inpainting_transformer_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"image_inpainting_transformer_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ patch_embedding_1               │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">918,016</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PatchEmbedding</span>)                │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_4             │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">6,302,720</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_5             │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">6,302,720</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_6             │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">6,302,720</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_7             │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">6,302,720</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Decoder</span>)             │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">393,984</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ patch_embedding_1               │ ?                      │       \u001b[38;5;34m918,016\u001b[0m │\n",
       "│ (\u001b[38;5;33mPatchEmbedding\u001b[0m)                │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_4             │ ?                      │     \u001b[38;5;34m6,302,720\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_5             │ ?                      │     \u001b[38;5;34m6,302,720\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_6             │ ?                      │     \u001b[38;5;34m6,302,720\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_7             │ ?                      │     \u001b[38;5;34m6,302,720\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ decoder_1 (\u001b[38;5;33mDecoder\u001b[0m)             │ ?                      │       \u001b[38;5;34m393,984\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,522,880</span> (101.18 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m26,522,880\u001b[0m (101.18 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,522,880</span> (101.18 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m26,522,880\u001b[0m (101.18 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class PatchEmbedding(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__(dtype=FLOAT)  # wtf\n",
    "        self.proj = keras.layers.Dense(D_model, dtype=FLOAT)  # (P² * C) -> D_model\n",
    "        self.positional_embedding = self.add_weight(\n",
    "            shape=(1, N, D_model),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "            name=\"positional_embedding\",\n",
    "            dtype=FLOAT,\n",
    "        )\n",
    "\n",
    "    # def build(self, input_shape):\n",
    "    #     self.call(tf.zeros((BS, N , P**2 * C)))\n",
    "    #     self.built = True\n",
    "\n",
    "    def call(self, patches_flat: tf.Tensor):\n",
    "        # R^{N x (P^2 . C)} -> R^{N x D_model}\n",
    "        assert patches_flat.dtype == FLOAT\n",
    "        X = self.proj(patches_flat)  # (N, D_model)\n",
    "        assert X.dtype == FLOAT\n",
    "        X += self.positional_embedding  # (N, D_model)\n",
    "        assert X.dtype == FLOAT\n",
    "        return X\n",
    "\n",
    "\n",
    "class MultiHeadAttention(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__(dtype=FLOAT)\n",
    "        # Project to h * D_head dimensions\n",
    "        self.W_Q = keras.layers.Dense(h * D_head, dtype=FLOAT)\n",
    "        self.W_K = keras.layers.Dense(h * D_head, dtype=FLOAT)\n",
    "        self.W_V = keras.layers.Dense(h * D_head, dtype=FLOAT)\n",
    "        # Project back to D_model\n",
    "        self.W_O = keras.layers.Dense(D_model, dtype=FLOAT)\n",
    "\n",
    "    def call(self, X, A):\n",
    "        # X: R^{BS x N x D_model}\n",
    "        # A: R^{BS x N x N}\n",
    "        # returns: R^{BS x N x D_model}\n",
    "\n",
    "        # In the standard implementation, each head has its own separate projection matrices. However, a common optimization is to project the input into h * D_head dimensions (which is D_model) with a single large projection, then split into h heads. So, if D_model = h * D_head, then using a Dense(D_model) for Q, K, V and then splitting into h heads each of D_head is equivalent to having h separate projections. This is a standard approach because it's more efficient to compute all heads in parallel with a single matrix multiplication rather than h separate ones.\n",
    "        # So the optimal way is to use combined projections.\n",
    "        Q = self.W_Q(X)  # (BS, N, h * D_head)\n",
    "        K = self.W_K(X)  # (BS, N, h * D_head)\n",
    "        V = self.W_V(X)  # (BS, N, h * D_head)\n",
    "        \n",
    "\n",
    "        Q = tf.reshape(Q, (-1, N, h, D_head))  # (BS, N, h, D_head)\n",
    "        K = tf.reshape(K, (-1, N, h, D_head))\n",
    "        V = tf.reshape(V, (-1, N, h, D_head))\n",
    "\n",
    "        # Transpose for attention computation\n",
    "        Q = tf.transpose(Q, [0, 2, 1, 3])  # (BS, h, N, D_head)\n",
    "        K = tf.transpose(K, [0, 2, 1, 3])\n",
    "        V = tf.transpose(V, [0, 2, 1, 3])\n",
    "        # scaled dot-product attention\n",
    "        attn_scores = tf.matmul(Q, K, transpose_b=True)  # (BS, h, N, N)\n",
    "        attn_scores /= tf.math.sqrt(\n",
    "            tf.cast(D_head, attn_scores.dtype)\n",
    "        )  # scale by sqrt(D_head)\n",
    "\n",
    "        A = tf.expand_dims(A, 1)  # (BS, 1, N, N)\n",
    "        attn_scores += A  # Broadcast to all heads\n",
    "\n",
    "        attn_weights = tf.nn.softmax(attn_scores, axis=-1)  # (BS, h, N, N)\n",
    "\n",
    "        output = tf.matmul(attn_weights, V)  # (BS, h, N, D_head)\n",
    "        output = tf.transpose(output, [0, 2, 1, 3])  # (BS, N, h, D_head)\n",
    "        output = tf.reshape(output, (-1, N, h * D_head))  # (BS, N, h * D_head)\n",
    "        output = self.W_O(output)  # (BS, N, D_model)\n",
    "        return output\n",
    "\n",
    "\n",
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__(dtype=FLOAT)\n",
    "        self.attn = MultiHeadAttention()\n",
    "        self.norm1 = keras.layers.LayerNormalization(dtype=FLOAT)\n",
    "        self.norm2 = keras.layers.LayerNormalization(dtype=FLOAT)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                keras.layers.Dense(D_fcn, activation=\"gelu\", dtype=FLOAT),\n",
    "                keras.layers.Dense(D_model, dtype=FLOAT),\n",
    "                keras.layers.Dropout(0.1, dtype=FLOAT),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    def call(self, X, A):\n",
    "        \"R^{N x D_model} -> R^{N x D_model}\"\n",
    "        X = self.norm1(X + self.attn(X, A))\n",
    "        X = self.norm2(X + self.ffn(X))\n",
    "        return X\n",
    "\n",
    "\n",
    "class Decoder(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__(dtype=FLOAT)\n",
    "        self.proj = keras.layers.Dense(P * P * C, dtype=FLOAT)\n",
    "\n",
    "    def call(self, X):\n",
    "        \"R^{BS x N x D_model} -> R^{BS x N x P x P x C}\"\n",
    "        BS = tf.shape(X)[0]\n",
    "        X = self.proj(X)\n",
    "        X = tf.reshape(X, (BS, N, P, P, C))\n",
    "        return X\n",
    "\n",
    "\n",
    "class ImageInpaintingTransformer(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__(dtype=FLOAT)\n",
    "        self.embed = PatchEmbedding()\n",
    "        self.transformer_blocks = [TransformerBlock() for _ in range(num_layers)]\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        BS = input_shape[0]\n",
    "        dummy_images = tf.zeros((BS, H, W, C), dtype=FLOAT)  # THIS WASTED 40 MINUTES\n",
    "        dummy_masks = tf.stack([random_visibility_mask() for _ in range(BS)])\n",
    "        self.call(dummy_images, dummy_masks)\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, image, mask):\n",
    "        patches = extract_patches(image)\n",
    "        mask_pooled, A = process_mask(mask)\n",
    "        BS = tf.shape(patches)[0]\n",
    "        patches_flat = tf.reshape(patches, [BS, N, P**2 * C])\n",
    "        # tf.print(tf.shape(patches_flat))\n",
    "        X = self.embed(patches_flat)\n",
    "        for block in self.transformer_blocks:\n",
    "            X = block(X, A)\n",
    "        reconstructed_patches = self.decoder(X)  # R^{BS x N x P x P x C}\n",
    "        return patches_to_imgs(reconstructed_patches)\n",
    "\n",
    "model = ImageInpaintingTransformer()\n",
    "model.build((BS, H, W, C))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tests patching and depatching\n",
    "\n",
    "# img = next(iter(dataset.take(1)))\n",
    "\n",
    "# viz_img(img)\n",
    "# patched = extract_patches(tf.expand_dims(img ,0))\n",
    "# tf.print(tf.shape(patched))\n",
    "\n",
    "# recreated_img = patches_to_imgs(patched)\n",
    "# viz_img(recreated_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sample(image):\n",
    "    mask = random_visibility_mask()\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "ds_masks = dataset.map(prepare_sample)\n",
    "train_count = int(valid_count * 0.8)\n",
    "test_count = int(valid_count * 0.1)\n",
    "val_count = valid_count - train_count - test_count\n",
    "\n",
    "train_ds = ds_masks.take(train_count).batch(BS)\n",
    "test_ds = ds_masks.skip(train_count).take(test_count).batch(BS)\n",
    "val_ds = ds_masks.skip(train_count + test_count).take(val_count).batch(BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/1000 [00:21<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node image_inpainting_transformer_1_1/transformer_block_7_1/multi_head_attention_7_1/Softmax defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/asyncio/base_events.py\", line 640, in run_forever\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/asyncio/base_events.py\", line 1992, in _run_once\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/asyncio/events.py\", line 88, in _run\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/tmp/ipykernel_40186/1663883276.py\", line 38, in <module>\n\n  File \"/tmp/ipykernel_40186/1663883276.py\", line 16, in train_step\n\n  File \"/home/navid/.pyenv/versions/3.12.8/envs/tf312/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/navid/.pyenv/versions/3.12.8/envs/tf312/lib/python3.12/site-packages/keras/src/layers/layer.py\", line 899, in __call__\n\n  File \"/home/navid/.pyenv/versions/3.12.8/envs/tf312/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/navid/.pyenv/versions/3.12.8/envs/tf312/lib/python3.12/site-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/home/navid/.pyenv/versions/3.12.8/envs/tf312/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/tmp/ipykernel_40186/3265770562.py\", line 131, in call\n\n  File \"/home/navid/.pyenv/versions/3.12.8/envs/tf312/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/navid/.pyenv/versions/3.12.8/envs/tf312/lib/python3.12/site-packages/keras/src/layers/layer.py\", line 899, in __call__\n\n  File \"/home/navid/.pyenv/versions/3.12.8/envs/tf312/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/navid/.pyenv/versions/3.12.8/envs/tf312/lib/python3.12/site-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/home/navid/.pyenv/versions/3.12.8/envs/tf312/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/tmp/ipykernel_40186/3265770562.py\", line 91, in call\n\n  File \"/home/navid/.pyenv/versions/3.12.8/envs/tf312/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/navid/.pyenv/versions/3.12.8/envs/tf312/lib/python3.12/site-packages/keras/src/layers/layer.py\", line 899, in __call__\n\n  File \"/home/navid/.pyenv/versions/3.12.8/envs/tf312/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/navid/.pyenv/versions/3.12.8/envs/tf312/lib/python3.12/site-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/home/navid/.pyenv/versions/3.12.8/envs/tf312/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/tmp/ipykernel_40186/3265770562.py\", line 66, in call\n\nOOM when allocating tensor with shape[4,16,1024,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node image_inpainting_transformer_1_1/transformer_block_7_1/multi_head_attention_7_1/Softmax}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_step_15190]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(train_ds, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m, total\u001b[38;5;241m=\u001b[39mtrain_count \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m BS)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_batch, mask_batch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m---> 38\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     40\u001b[0m     steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/envs/tf312/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/envs/tf312/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node image_inpainting_transformer_1_1/transformer_block_7_1/multi_head_attention_7_1/Softmax defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/asyncio/base_events.py\", line 640, in run_forever\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/asyncio/base_events.py\", line 1992, in _run_once\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/asyncio/events.py\", line 88, in _run\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/home/navid/.pyenv/versions/3.12.8/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/tmp/ipykernel_40186/1663883276.py\", line 38, in <module>\n\n  File \"/tmp/ipykernel_40186/1663883276.py\", line 16, in train_step\n\n  File \"/home/navid/.pyenv/versions/3.12.8/envs/tf312/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/navid/.pyenv/versions/3.12.8/envs/tf312/lib/python3.12/site-packages/keras/src/layers/layer.py\", line 899, in __call__\n\n  File \"/home/navid/.pyenv/versions/3.12.8/envs/tf312/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/navid/.pyenv/versions/3.12.8/envs/tf312/lib/python3.12/site-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/home/navid/.pyenv/versions/3.12.8/envs/tf312/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/tmp/ipykernel_40186/3265770562.py\", line 131, in call\n\n  File \"/home/navid/.pyenv/versions/3.12.8/envs/tf312/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/navid/.pyenv/versions/3.12.8/envs/tf312/lib/python3.12/site-packages/keras/src/layers/layer.py\", line 899, in __call__\n\n  File \"/home/navid/.pyenv/versions/3.12.8/envs/tf312/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/navid/.pyenv/versions/3.12.8/envs/tf312/lib/python3.12/site-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/home/navid/.pyenv/versions/3.12.8/envs/tf312/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/tmp/ipykernel_40186/3265770562.py\", line 91, in call\n\n  File \"/home/navid/.pyenv/versions/3.12.8/envs/tf312/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/navid/.pyenv/versions/3.12.8/envs/tf312/lib/python3.12/site-packages/keras/src/layers/layer.py\", line 899, in __call__\n\n  File \"/home/navid/.pyenv/versions/3.12.8/envs/tf312/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/navid/.pyenv/versions/3.12.8/envs/tf312/lib/python3.12/site-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/home/navid/.pyenv/versions/3.12.8/envs/tf312/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/tmp/ipykernel_40186/3265770562.py\", line 66, in call\n\nOOM when allocating tensor with shape[4,16,1024,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node image_inpainting_transformer_1_1/transformer_block_7_1/multi_head_attention_7_1/Softmax}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_step_15190]"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def l1_masked_loss(y_true: tf.Tensor, y_pred: tf.Tensor, obsvmask: tf.Tensor):\n",
    "    inpaintmask = tf.logical_not(obsvmask)\n",
    "    whole_image_diff = tf.subtract(y_true, y_pred)\n",
    "    area = mask_area(inpaintmask)\n",
    "    masked_diff = tf.multiply(whole_image_diff, tf.cast(inpaintmask, FLOAT))\n",
    "    return tf.reduce_mean(tf.abs(masked_diff))\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(image: tf.Tensor, mask: tf.Tensor):\n",
    "    with tf.GradientTape() as tape:\n",
    "        reconstructed_img = model(image, mask)  # N x P x P x C\n",
    "        loss = l1_masked_loss(image, reconstructed_img, mask)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def val_step(image: tf.Tensor, mask: tf.Tensor):\n",
    "    reconstructed_img = model(image, mask)\n",
    "    loss = l1_masked_loss(image, reconstructed_img, mask)\n",
    "    return loss\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "print(\"Starting training\")\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    steps = 0\n",
    "    pbar = tqdm(train_ds, desc=f\"Epoch {epoch+1}\", unit=\"batch\", total=train_count // BS)\n",
    "    for image_batch, mask_batch in pbar:\n",
    "        loss = train_step(image_batch, mask_batch)\n",
    "        epoch_loss += loss\n",
    "        steps += 1\n",
    "        # Dynamically update the tqdm bar without spamming stdout\n",
    "        pbar.set_postfix(loss=f\"{loss:.4f}\")\n",
    "    train_loss = epoch_loss / steps\n",
    "\n",
    "    val_loss_total = 0.0\n",
    "    val_steps = 0\n",
    "    pbar_val = tqdm(val_ds, desc=f\"Epoch {epoch+1} Validation\", unit=\"batch\", total=len(val_ds))\n",
    "    for val_image_batch, val_mask_batch in pbar_val:\n",
    "        loss = val_step(val_image_batch, val_mask_batch)\n",
    "        val_loss_total += loss\n",
    "        val_steps += 1\n",
    "        pbar_val.set_postfix(loss=f\"{loss:.4f}\")\n",
    "    avg_val_loss = val_loss_total / val_steps\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Summary: Train Loss = {train_loss:.4f} | Validation Loss = {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_obsv_mask(image: tf.Tensor, obvmask: tf.Tensor) -> tf.Tensor:\n",
    "    return tf.multiply(image, tf.cast(obvmask, FLOAT))\n",
    "\n",
    "\n",
    "def viz_grid(batch: tf.Tensor):\n",
    "    batch_size: int = batch.shape[0]  # type: ignore\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=batch_size, figsize=(5, 5))\n",
    "    for i in range(batch_size):\n",
    "        # Original image\n",
    "        axes[i].imshow(tf.cast(batch[i], dtype=tf.float32).numpy())  # type: ignore\n",
    "        axes[i].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def reconstruct(original: tf.Tensor, reconstruct: tf.Tensor, obvmask: tf.Tensor):\n",
    "    return tf.add(\n",
    "        tf.multiply(tf.cast(obvmask, FLOAT), original),\n",
    "        tf.multiply(tf.cast(tf.logical_not(obvmask), FLOAT), reconstruct),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualitative Eval\n",
    "# visualize_unbatched_dataset(test_ds, 5)\n",
    "\n",
    "img, obvmask = next(iter(test_ds.take(1)))\n",
    "# viz_grid(img)\n",
    "viz_grid(apply_obsv_mask(img, obvmask))\n",
    "reconstructed = reconstruct(img, model(img, obvmask), obvmask)\n",
    "viz_grid(reconstructed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
