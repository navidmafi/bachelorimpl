{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# terminology:\n",
    "# obs_mask:  1 = observed,   0 = hole\n",
    "# hole_mask: 1 = hole,       0 = not hole (observed)\n",
    "\n",
    "\n",
    "# Absolutely lovely state of typesafety in this godforsaken ecosystem. We will use underscores a lot ...\n",
    "# I will try to minimize usage of tf.cast\n",
    "\n",
    "\n",
    "# we place emphasis on determinism\n",
    "\n",
    "# only change one parameter (optimizer/dataset/hyperparams/overall architecture)in experiments\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "import time,pytz\n",
    "import os, glob, subprocess, sys, uuid\n",
    "from tqdm import tqdm\n",
    "\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "from psycopg2.extras import Json\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"mldb\", user=\"navid\", password=\"123\", host=\"pg.network.navidmafi.com\", port=\"443\",\n",
    ")\n",
    "conn.autocommit = True\n",
    "cur = conn.cursor()\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"42\"\n",
    "os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"1\"\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.InteractiveSession(config=config)\n",
    "\n",
    "import keras\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "GLOBAL_RANDOM_SEED = 42\n",
    "\n",
    "random.seed(GLOBAL_RANDOM_SEED)\n",
    "np.random.seed(GLOBAL_RANDOM_SEED)\n",
    "tf.random.set_seed(GLOBAL_RANDOM_SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "initializer = keras.initializers.GlorotUniform(seed=GLOBAL_RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_TO_TRAIN=40\n",
    "num_samples = 5000\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "H = 64\n",
    "W = 64\n",
    "C = 3\n",
    "\n",
    "P = 8\n",
    "assert H == W\n",
    "assert H % P == 0\n",
    "\n",
    "h = 8\n",
    "\n",
    "D_model = 1024\n",
    "D_head = 128\n",
    "D_fcn = 1024\n",
    "num_layers = 8\n",
    "N = (H * W) // (P * P)\n",
    "\n",
    "MASK_MAX_SIZE = 20\n",
    "MASK_MIN_SIZE = 20\n",
    "assert MASK_MAX_SIZE < H/2\n",
    "\n",
    "optimizer = keras.optimizers.AdamW(learning_rate=1e-4, weight_decay=1e-2)\n",
    "# optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "# keras.mixed_precision.set_global_policy(\"mixed_bfloat16\")\n",
    "FLOAT = tf.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_img(img):\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    plt.imshow(tf.squeeze(img).numpy(), cmap=\"gray\")\n",
    "    plt.colorbar()\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    plt.text(\n",
    "        0.99,\n",
    "        0.01,\n",
    "        timestamp,\n",
    "        ha=\"right\",\n",
    "        va=\"bottom\",\n",
    "        transform=plt.gca().transAxes,\n",
    "        fontsize=12,\n",
    "        color=\"white\",\n",
    "        alpha=0.9,\n",
    "        bbox=dict(boxstyle=\"square,pad=0.1\", facecolor=\"black\", alpha=0.3),\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def viz_mask(mask):\n",
    "    plt.imshow(tf.squeeze(mask).numpy(), cmap=\"gray\", vmin=0, vmax=1)\n",
    "    plt.colorbar()\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    plt.text(\n",
    "        0.99,\n",
    "        0.01,\n",
    "        timestamp,\n",
    "        ha=\"right\",\n",
    "        va=\"bottom\",\n",
    "        transform=plt.gca().transAxes,\n",
    "        fontsize=12,\n",
    "        color=\"white\",\n",
    "        alpha=0.9,\n",
    "        bbox=dict(boxstyle=\"square,pad=0.1\", facecolor=\"black\", alpha=0.3),\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def viz_grid(batch: tf.Tensor, max: int = 4):\n",
    "    batch_size: int = batch.shape[0]  # type: ignore\n",
    "    num = min(batch_size, max)\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=num, figsize=(15, 15), dpi=300)\n",
    "    if num == 1:\n",
    "        axes = [axes]\n",
    "    for i in range(num):\n",
    "        # Original image\n",
    "        axes[i].imshow(\n",
    "            tf.clip_by_value(\n",
    "                tf.cast(batch[i], dtype=tf.float32), 0, 1  # type: ignore\n",
    "            ).numpy()  # type: ignore\n",
    "        )\n",
    "        axes[i].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def random_obs_mask(H, W, MASK_MIN_SIZE, MASK_MAX_SIZE):\n",
    "    \"-> [0,1]^{H x W x C}, dtype=FLOAT\"\n",
    "    w, h = [np.random.randint(MASK_MIN_SIZE, MASK_MAX_SIZE + 1) for _ in range(2)]\n",
    "    x = np.random.randint(0, W - w + 1)\n",
    "    y = np.random.randint(0, H - h + 1)\n",
    "\n",
    "    # Start with all ones (observed pixels)\n",
    "    mask = tf.ones((H, W), dtype=FLOAT)\n",
    "\n",
    "    ys = tf.range(y, y + h)\n",
    "    xs = tf.range(x, x + w)\n",
    "    yy, xx = tf.meshgrid(ys, xs, indexing=\"ij\")\n",
    "    indices = tf.stack([yy, xx], axis=-1)\n",
    "    indices = tf.reshape(indices, (-1, 2))\n",
    "\n",
    "    # Set the hole region to zero\n",
    "    updates = tf.zeros((h * w,), dtype=FLOAT)\n",
    "    mask = tf.tensor_scatter_nd_update(mask, indices, updates)\n",
    "\n",
    "    return tf.expand_dims(mask, -1)\n",
    "\n",
    "\n",
    "def load_and_validate(file_path, ds_shape_advertised):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_image(\n",
    "        img, channels=ds_shape_advertised[2], expand_animations=False\n",
    "    )\n",
    "    img = tf.divide(tf.cast(img, dtype=FLOAT), 255.0)\n",
    "    is_valid = tf.reduce_all(tf.equal(tf.shape(img), tf.constant(ds_shape_advertised)))\n",
    "\n",
    "    return img, is_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_shape_advertised = (512, 512, 3)\n",
    "dataset_path = \"/mnt/Data/ML/datasets/portraits\"\n",
    "\n",
    "\n",
    "all_files = [\n",
    "    os.path.join(dataset_path, f)\n",
    "    for f in os.listdir(dataset_path)\n",
    "    if f.endswith((\".jpg\", \".png\"))\n",
    "]\n",
    "random.shuffle(all_files)\n",
    "selected_files = all_files[:num_samples]\n",
    "dataset = tf.data.Dataset.from_tensor_slices(selected_files)\n",
    "dataset = dataset.map(lambda img: load_and_validate(img, ds_shape_advertised))\n",
    "dataset = dataset.filter(lambda img, is_valid: is_valid)\n",
    "dataset = dataset.map(lambda img, is_valid: img)\n",
    "dataset = dataset.map(lambda img: tf.image.resize(img, (H, W)))\n",
    "\n",
    "# tf.print(next(iter(dataset.take(1)))[0].dtype)\n",
    "# with tf.device(\"/cpu:0\"):\n",
    "valid_count = dataset.reduce(\n",
    "    tf.constant(0, dtype=tf.int32), lambda x, _: x + 1\n",
    ").numpy()  # type: ignore\n",
    "\n",
    "print(f\"Valid images count: {valid_count}\")\n",
    "assert valid_count, \"Everything's gone\"\n",
    "\n",
    "masks = [\n",
    "    random_obs_mask(H, W, MASK_MIN_SIZE, MASK_MAX_SIZE) for _ in range(valid_count)\n",
    "]\n",
    "mask_ds = tf.data.Dataset.from_tensor_slices(masks)\n",
    "\n",
    "ds = tf.data.Dataset.zip((dataset, mask_ds))\n",
    "\n",
    "\n",
    "train_count = int(valid_count * 0.8)\n",
    "test_count = int(valid_count * 0.1)\n",
    "val_count = valid_count - train_count - test_count\n",
    "\n",
    "train_ds = ds.take(train_count).batch(BATCH_SIZE)\n",
    "test_ds = ds.skip(train_count).take(test_count).batch(BATCH_SIZE)\n",
    "val_ds = ds.skip(train_count + test_count).take(val_count).batch(BATCH_SIZE)\n",
    "\n",
    "train_batches = -(train_count // -BATCH_SIZE)\n",
    "test_batches = -(test_count // -BATCH_SIZE)\n",
    "val_batches = -(val_count // -BATCH_SIZE)\n",
    "\n",
    "print(\"Trn,Tst,Val:\", train_count, test_count, val_count)\n",
    "print(\"Trn,Tst,Val batches:\", train_batches, test_batches, val_batches)\n",
    "viz_grid(next(iter(train_ds.take(1).map(lambda img, mask: img))), 8)  # type: ignore\n",
    "viz_grid(next(iter(train_ds.take(1).map(lambda img, mask: mask))), 8)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches(image: tf.Tensor) -> tf.Tensor:\n",
    "    \"R^{BS x H x W x C} -> R^{BS x N x P^2 x C}\"\n",
    "\n",
    "    patches: tf.Tensor = tf.image.extract_patches(\n",
    "        images=image,\n",
    "        sizes=[1, P, P, 1],\n",
    "        strides=[1, P, P, 1],\n",
    "        rates=[1, 1, 1, 1],\n",
    "        padding=\"VALID\",\n",
    "    )\n",
    "    BS, H_prime, W_prime, _ = tf.unstack(tf.shape(patches))\n",
    "\n",
    "    # Reshape patches to [BS, H' * W', P*P, C]\n",
    "    patches = tf.reshape(patches, [BS, H_prime * W_prime, P * P, -1])\n",
    "\n",
    "    return patches\n",
    "\n",
    "\n",
    "def patches_to_imgs(patches: tf.Tensor) -> tf.Tensor:\n",
    "    \"R^{BS x N x P.P.C} -> R^{BS x H x W x C}\"\n",
    "    BS = tf.shape(patches)[0]\n",
    "    grid_size = H // P  # same as W // P\n",
    "    patches = tf.reshape(patches, [BS, grid_size, grid_size, P, P, C])\n",
    "    patches = tf.transpose(patches, perm=[0, 1, 3, 2, 4, 5])\n",
    "    image = tf.reshape(patches, [BS, grid_size * P, grid_size * P, C])\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "sample = tf.expand_dims(next(iter(dataset.take(1))), 0)\n",
    "tf.assert_equal(patches_to_imgs(extract_patches(sample)), sample)\n",
    "\n",
    "\n",
    "def create_attention_mask(obs_mask_bool: tf.Tensor):\n",
    "    \"R^{BS x H x W} -> R^{BS x N x N}\"\n",
    "    # TF does not support native min pooling.\n",
    "    # The mask shown is OBSERVATION MASK meaning 0 means missing.\n",
    "\n",
    "    BS = tf.shape(obs_mask_bool)[0]\n",
    "    mask_pooled = tf.nn.max_pool2d(\n",
    "        tf.cast(tf.logical_not(obs_mask_bool), dtype=tf.int8),\n",
    "        ksize=[P, P],\n",
    "        strides=[P, P],\n",
    "        padding=\"VALID\",\n",
    "    )\n",
    "    mask_pooled = tf.logical_not(tf.cast(mask_pooled, tf.bool))\n",
    "    # viz_mask(mask_pooled)\n",
    "    mask_pooled = tf.reshape(mask_pooled, [BS, N])\n",
    "    mask_expanded = tf.expand_dims(mask_pooled, axis=1)  # (BS, 1, N)\n",
    "    mask_expanded = tf.tile(mask_expanded, [1, N, 1])  # (BS, N, N)\n",
    "    A = tf.where(\n",
    "        mask_expanded,\n",
    "        tf.constant(0.0, dtype=FLOAT),  # zero penanly\n",
    "        tf.constant(-float(\"inf\"), dtype=FLOAT),  # inf penalty\n",
    "    )\n",
    "    return A\n",
    "\n",
    "\n",
    "def apply_obsv_mask(image: tf.Tensor, obs_mask_float: tf.Tensor) -> tf.Tensor:\n",
    "    return tf.multiply(image, obs_mask_float)\n",
    "\n",
    "\n",
    "def reconstruct(\n",
    "    original: tf.Tensor, reconstruct: tf.Tensor, obs_mask_float: tf.Tensor\n",
    ") -> tf.Tensor:\n",
    "    obs_mask_bool = tf.cast(obs_mask_float, dtype=tf.bool)\n",
    "    return tf.add(\n",
    "        tf.multiply(tf.cast(obs_mask_bool, FLOAT), original),\n",
    "        tf.multiply(tf.cast(tf.logical_not(obs_mask_bool), FLOAT), reconstruct),\n",
    "    )\n",
    "\n",
    "\n",
    "sample = next(iter(train_ds.unbatch().take(1)))\n",
    "print(sample[0].dtype)\n",
    "viz_img(sample[0])\n",
    "# viz_mask(sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commonDense = {\"dtype\": FLOAT, \"kernel_initializer\": initializer}\n",
    "\n",
    "\n",
    "class PatchEmbedding(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__(dtype=FLOAT)\n",
    "        self.proj = keras.layers.Dense(D_model, **commonDense)  # (PÂ² * C) -> D_model\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.positional_embedding = self.add_weight(\n",
    "            shape=(N, D_model), initializer=initializer, name=\"pos_embed\"\n",
    "        )\n",
    "\n",
    "    def call(self, patches_flat: tf.Tensor):\n",
    "        # R^{BS x N x (P^2 . C)} -> R^{BS x N x D_model}\n",
    "        X = self.proj(patches_flat)\n",
    "        X += self.positional_embedding\n",
    "        return X\n",
    "\n",
    "\n",
    "class MultiHeadAttention(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__(dtype=FLOAT)\n",
    "        # Project to h * D_head dimensions\n",
    "        self.W_Q = keras.layers.Dense(h * D_head, **commonDense)\n",
    "        self.W_K = keras.layers.Dense(h * D_head, **commonDense)\n",
    "        self.W_V = keras.layers.Dense(h * D_head, **commonDense)\n",
    "        # Project back to D_model\n",
    "        self.W_O = keras.layers.Dense(D_model, **commonDense)\n",
    "\n",
    "    def call(self, X, A):\n",
    "        \"\"\"\n",
    "        X: R^{BS x N x D_model}, A: R^{BS x N x N} -> R^{BS x N x D_model}\n",
    "        \"\"\"\n",
    "\n",
    "        # In the standard implementation, each head has its own separate projection matrices. However, a common optimization is to project the input into h * D_head dimensions (which is D_model) with a single large projection, then split into h heads. So, if D_model = h * D_head, then using a Dense(D_model) for Q, K, V and then splitting into h heads each of D_head is equivalent to having h separate projections. This is a standard approach because it's more efficient to compute all heads in parallel with a single matrix multiplication rather than h separate ones.\n",
    "        # So the optimal way is to use combined projections.\n",
    "        Q = self.W_Q(X)  # (BS, N, h * D_head)\n",
    "        K = self.W_K(X)  # (BS, N, h * D_head)\n",
    "        V = self.W_V(X)  # (BS, N, h * D_head)\n",
    "\n",
    "        Q = tf.reshape(Q, (-1, N, h, D_head))  # (BS, N, h, D_head)\n",
    "        K = tf.reshape(K, (-1, N, h, D_head))\n",
    "        V = tf.reshape(V, (-1, N, h, D_head))\n",
    "\n",
    "        # Transpose for attention computation\n",
    "        Q = tf.transpose(Q, [0, 2, 1, 3])  # (BS, h, N, D_head)\n",
    "        K = tf.transpose(K, [0, 2, 1, 3])\n",
    "        V = tf.transpose(V, [0, 2, 1, 3])\n",
    "        # scaled dot-product attention\n",
    "        attn_scores = tf.matmul(Q, K, transpose_b=True)  # (BS, h, N, N)\n",
    "        attn_scores /= tf.math.sqrt(\n",
    "            tf.cast(D_head, attn_scores.dtype)\n",
    "        )  # scale by sqrt(D_head)\n",
    "\n",
    "        A = tf.expand_dims(A, 1)  # (BS, 1, N, N)\n",
    "        attn_scores += A  # Broadcast to all heads\n",
    "\n",
    "        attn_weights = tf.nn.softmax(attn_scores, axis=-1)  # (BS, h, N, N)\n",
    "\n",
    "        output = tf.matmul(attn_weights, V)  # (BS, h, N, D_head)\n",
    "        output = tf.transpose(output, [0, 2, 1, 3])  # (BS, N, h, D_head)\n",
    "        output = tf.reshape(output, (-1, N, h * D_head))  # (BS, N, h * D_head)\n",
    "        output = self.W_O(output)  # (BS, N, D_model)\n",
    "        return output\n",
    "\n",
    "\n",
    "class KMultiHeadAttention(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__(dtype=FLOAT)\n",
    "        self.mha = keras.layers.MultiHeadAttention(\n",
    "            num_heads=h,\n",
    "            key_dim=D_head,\n",
    "            output_shape=D_model,\n",
    "            use_bias=True,\n",
    "            dtype=FLOAT,\n",
    "        )\n",
    "\n",
    "    def call(self, X, attn_mask):\n",
    "        \"R^{BS x N x D_model}, R^{BS x N x N} -> R^{BS x N x D_model}\"\n",
    "        return self.mha(X, X, attention_mask=attn_mask)\n",
    "\n",
    "\n",
    "class TransformerBlock(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__(dtype=FLOAT)\n",
    "        self.attn = KMultiHeadAttention()\n",
    "        self.norm1 = keras.layers.LayerNormalization(dtype=FLOAT)\n",
    "        self.norm2 = keras.layers.LayerNormalization(dtype=FLOAT)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                keras.layers.Dense(\n",
    "                    D_fcn, activation=\"relu\", **commonDense\n",
    "                ),  # Switched to ReLU\n",
    "                keras.layers.Dense(D_model, **commonDense),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def call(self, X, A):\n",
    "        \"R^{BS x N x D_model} -> R^{BS x N x D_model}\"\n",
    "\n",
    "        A = tf.cast(A, dtype=tf.bool)\n",
    "        # NEW : pre norm blocks\n",
    "        X_norm = self.norm1(X)\n",
    "        X_attn = self.attn(X_norm, A)\n",
    "        X = X + X_attn\n",
    "        X_norm2 = self.norm2(X)\n",
    "        X_ffn = self.ffn(X_norm2)\n",
    "        X = X + X_ffn\n",
    "        return X\n",
    "\n",
    "\n",
    "class Refiner(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__(dtype=FLOAT)\n",
    "        self.conv1 = keras.layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\")\n",
    "        self.conv2 = keras.layers.Conv2D(\n",
    "            128, 3, dilation_rate=2, padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv3 = keras.layers.Conv2D(3, 3, padding=\"same\")\n",
    "\n",
    "    def call(self, X):\n",
    "        \"R^{BS x H x W x C} -> R^{BS x H x W x C}\"\n",
    "        residual = X\n",
    "        X = self.conv1(X)\n",
    "        X = self.conv2(X)\n",
    "        X = self.conv3(X)\n",
    "        return X + residual\n",
    "\n",
    "\n",
    "class TransformerDecoder(keras.layers.Layer):\n",
    "    \"R^{BS x N x D_model} -> R^{BS x N x P*P*C}\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(dtype=FLOAT)\n",
    "        self.proj1 = keras.layers.Dense(D_model, activation=\"gelu\", **commonDense)\n",
    "        self.proj2 = keras.layers.Dense(P * P * C, activation=\"sigmoid\", **commonDense)\n",
    "\n",
    "    def call(self, X):\n",
    "        BS = tf.shape(X)[0]\n",
    "        X = self.proj1(X)\n",
    "        X = self.proj2(X)\n",
    "        return tf.reshape(X, (BS, N, P, P, C))\n",
    "\n",
    "\n",
    "class ImageInpaintingTransformer(keras.Model):\n",
    "    \"\"\"outputs R ^ {BS x H x W x C}\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(dtype=FLOAT)\n",
    "        self.embed = PatchEmbedding()\n",
    "        self.transformer_blocks = [TransformerBlock() for _ in range(num_layers)]\n",
    "        self.decoder = TransformerDecoder()\n",
    "        self.refiner = Refiner()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        BS = input_shape[0]\n",
    "        # dummy_images = tf.zeros((BS, H, W, C), dtype=FLOAT)  # THIS WASTED 40 MINUTES\n",
    "        self.call(*next(iter(val_ds.take(1))))\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, image, obs_mask_float):\n",
    "        image = tf.multiply(image, obs_mask_float)\n",
    "        obs_mask_bool = tf.cast(obs_mask_float, dtype=tf.bool)\n",
    "        # viz_img(image[0])\n",
    "        patches = extract_patches(image)\n",
    "        AttnMask = create_attention_mask(obs_mask_bool)\n",
    "        # viz_img(AttnMask[0])\n",
    "\n",
    "        BS = tf.shape(patches)[0]\n",
    "        patches_flat = tf.reshape(patches, [BS, N, P**2 * C])\n",
    "        # tf.print(tf.shape(patches_flat))\n",
    "        X = self.embed(patches_flat)\n",
    "        for block in self.transformer_blocks:\n",
    "            X = block(X, AttnMask)\n",
    "\n",
    "        decoded = self.decoder(X)\n",
    "        return patches_to_imgs(decoded)\n",
    "\n",
    "\n",
    "model = ImageInpaintingTransformer()\n",
    "model.build((BATCH_SIZE, H, W, C))\n",
    "# model.compute_output_shape((BATCH_SIZE, H, W, C))\n",
    "model.summary()\n",
    "\n",
    "run_id = uuid.uuid4()\n",
    "print(run_id)\n",
    "session_epochs = 0\n",
    "session_steps = 0\n",
    "\n",
    "\n",
    "def costfunc(\n",
    "    y_true: tf.Tensor, y_pred: tf.Tensor, obs_mask_float: tf.Tensor\n",
    ") -> tf.RaggedTensor:\n",
    "    errors = tf.square(tf.subtract(y_true, y_pred))\n",
    "    hole_mask = 1.0 - obs_mask_float  # type: ignore\n",
    "    \n",
    "    hole_loss = tf.reduce_sum(errors * hole_mask) / (tf.reduce_sum(hole_mask) + 1e-8)\n",
    "    valid_loss = tf.reduce_sum(errors * obs_mask_float) / (tf.reduce_sum(obs_mask_float) + 1e-8)\n",
    "\n",
    "    return 2 * hole_loss + 1 * valid_loss\n",
    "\n",
    "\n",
    "def train_step(img: tf.Tensor, mask: tf.Tensor) -> tf.RaggedTensor:\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = model(img, mask, training=True)\n",
    "        loss = costfunc(img, pred, mask)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    if gradients is None:\n",
    "        raise RuntimeError()\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def val_step(image: tf.Tensor, mask: tf.Tensor) -> tf.RaggedTensor:\n",
    "    reconstructed_img = model(image, mask, training=False)\n",
    "    loss = costfunc(image, reconstructed_img, mask)\n",
    "    return loss\n",
    "\n",
    "import inspect\n",
    "import textwrap\n",
    "loss_source = textwrap.dedent(inspect.getsource(costfunc))\n",
    "cur.execute(\n",
    "    \"INSERT INTO runs (run_id, started_at, hyperparams) VALUES (%s, %s, %s)\",\n",
    "    (\n",
    "        str(run_id),\n",
    "        datetime.now(tz=pytz.UTC),\n",
    "        Json(\n",
    "            {\n",
    "                \"num_samples\": num_samples,\n",
    "                \"optimizer\": optimizer.get_config(),\n",
    "                \"D_model\": D_model,\n",
    "                \"D_head\": D_head,\n",
    "                \"D_fcn\": D_fcn,\n",
    "                \"H\": H,\n",
    "                \"W\": W,\n",
    "                'P' : P,\n",
    "                \"heads\": h,\n",
    "                'Float': str(FLOAT),\n",
    "                \"num_layers\": num_layers,\n",
    "                \"batch_size\": BATCH_SIZE,\n",
    "                \"loss\": loss_source\n",
    "            }\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = \"decoder_unet_1.keras\"\n",
    "# model.load_weights(name)\n",
    "# model.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(session_epochs, int)\n",
    "assert isinstance(session_steps, int)\n",
    "print(\n",
    "    f\"Starting training for {EPOCHS_TO_TRAIN - session_epochs} \"\n",
    "    f\"epochs (already did {session_epochs})\"\n",
    ")\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_epoch = -1\n",
    "for _ in range(EPOCHS_TO_TRAIN - session_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    steps = 0\n",
    "    pbar = tqdm(\n",
    "        train_ds,\n",
    "        desc=f\"Epoch {session_epochs+1}\",\n",
    "        unit=\"step\",\n",
    "        total=train_batches,\n",
    "    )\n",
    "    for image_batch, mask_batch in pbar:\n",
    "        training_step_loss = train_step(image_batch, mask_batch).numpy()\n",
    "        epoch_loss += training_step_loss\n",
    "        steps += 1\n",
    "        session_steps += 1\n",
    "        pbar.set_postfix(loss=f\"{float(training_step_loss):.4f}\")\n",
    "    train_loss = epoch_loss / steps\n",
    "    cur.execute(\n",
    "        \"INSERT INTO metrics (run_id, epoch, phase, value, logged_at, metric) VALUES (%s, %s, %s, %s, %s, %s)\",\n",
    "        (\n",
    "            str(run_id),\n",
    "            int(session_epochs+1),\n",
    "            \"train\",\n",
    "            float(train_loss),\n",
    "            datetime.now(tz=pytz.UTC),\n",
    "            \"train\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    val_loss_total = 0.0\n",
    "    val_steps = 0\n",
    "    pbar_val = tqdm(\n",
    "        val_ds,\n",
    "        desc=f\"Epoch {session_epochs+1} Validation\",\n",
    "        unit=\"step\",\n",
    "        total=val_batches,\n",
    "    )\n",
    "    for val_image_batch, val_mask_batch in pbar_val:\n",
    "        val_step_loss = val_step(val_image_batch, val_mask_batch).numpy()\n",
    "        val_loss_total += val_step_loss\n",
    "        val_steps += 1\n",
    "        pbar_val.set_postfix(loss=f\"{float(val_step_loss):.4f}\")\n",
    "\n",
    "    val_loss = val_loss_total / val_steps\n",
    "\n",
    "    cur.execute(\n",
    "        \"INSERT INTO metrics (run_id, epoch, phase, value, logged_at, metric) VALUES (%s, %s, %s, %s, %s, %s)\",\n",
    "        (\n",
    "            str(run_id),\n",
    "            int(session_epochs+1),\n",
    "            \"val\",\n",
    "            float(val_loss),\n",
    "            datetime.now(tz=pytz.UTC),\n",
    "            \"val\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_epoch = session_epochs + 1\n",
    "        model.save(\"best_run.keras\")\n",
    "    print(\n",
    "        f\"Epoch {session_epochs+1} Summary:\\n Steps:{steps} | Train Loss = {train_loss:.4f} | Validation Loss = {val_loss:.4f}\"\n",
    "    )\n",
    "    session_epochs += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, obs_mask_float = next(iter(train_ds.take(1)))\n",
    "viz_grid(img)\n",
    "viz_grid(apply_obsv_mask(img, obs_mask_float))\n",
    "model_out = model(img, obs_mask_float)\n",
    "reconstructed = reconstruct(img, model(img, obs_mask_float), obs_mask_float)\n",
    "# viz_grid(reconstructed)\n",
    "viz_grid(model_out)\n",
    "\n",
    "# viz_img(model_out[0])\n",
    "# viz_img(img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualitative Eval\n",
    "# visualize_unbatched_dataset(test_ds, 5)\n",
    "\n",
    "\n",
    "# img = tf.image.decode_image(\n",
    "#     tf.io.read_file(\"/home/navid/Dev/PaperTex/impl/naruto\")\n",
    "#     , dtype=tf.float32)\n",
    "# img = tf.image.resize_with_crop_or_pad(img, H, W)\n",
    "# img = tf.expand_dims(img, 0)\n",
    "# tf.print(tf.shape(img))\n",
    "# obvmask = tf.expand_dims(random_visibility_mask(),0)\n",
    "# tf.print(tf.shape(obvmask))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf31211",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
